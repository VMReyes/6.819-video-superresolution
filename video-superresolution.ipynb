{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f8943e",
   "metadata": {},
   "source": [
    "# Video Super Resolution\n",
    "\n",
    "## By Daniel Shkreli and Victor Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd443c",
   "metadata": {},
   "source": [
    "### Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a277c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.57.0)\n",
      "Using the GPU!\n",
      "1.4.0\n",
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import lpips\n",
    "import torchvision\n",
    "from torchvision.datasets.video_utils import VideoClips\n",
    "from torchvision import datasets, models, transforms\n",
    "!pip install tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")\n",
    "    \n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b884ed7",
   "metadata": {},
   "source": [
    "### Import the dataset\n",
    "To do this, we will be downloading the videos of the National Geographic channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c00196",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We will use resnet 18, pretrained at first.\n",
    "from https://github.com/hsinyilin19/ResNetVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ee5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256\n",
    "data_transforms = {\n",
    "    'train_in': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'train_in_no_normalize': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'train_target': transforms.Compose([\n",
    "        transforms.Resize(input_size*2),\n",
    "        transforms.CenterCrop(input_size*2),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "def initialize_resnet_model(resume_from = None):\n",
    "    resnet = models.resnet18(pretrained=True)\n",
    "    out_size = resnet.fc.in_features\n",
    "    print(\"out_size:\", out_size)\n",
    "    layers = []\n",
    "    for child in resnet.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "        layers.append(child)\n",
    "    resnet = nn.Sequential(*layers[:-1])\n",
    "    \n",
    "    #resnet.fc =  # create latent\n",
    "    \n",
    "    #resnet.add_module(\"bottleneck\", nn.Sequential()\n",
    "    if resume_from is not None:\n",
    "        print(f\"Loading weights from {resume_from}\")\n",
    "        model.load_state_dict(torch.load(resume_from))\n",
    "    return resnet, out_size\n",
    "\n",
    "def initialize_bottleneck(input_size, latent_size=1024):\n",
    "    bottleneck = nn.Sequential(nn.Linear(input_size, latent_size),\n",
    "                               nn.BatchNorm1d(latent_size, momentum=0.01),\n",
    "                               nn.ReLU(inplace=True))\n",
    "    return bottleneck\n",
    "\n",
    "def initialize_decoder():\n",
    "    stride = (2,2)\n",
    "    padding = (1,1)\n",
    "    convTrans6 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=1024, out_channels=1024, kernel_size=(4,4), stride=(1,1),\n",
    "                           padding=(0,0)),\n",
    "        nn.BatchNorm2d(1024, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans7 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(512, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans8 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(256, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans9 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(128, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans10 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(64, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "   \n",
    "    convTrans11 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(64, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans12 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(32, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans13 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(3, momentum=0.01),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "    return nn.Sequential(convTrans6,\n",
    "                         convTrans7,\n",
    "                         convTrans8,\n",
    "                         convTrans9,\n",
    "                         convTrans10,\n",
    "                         convTrans11,\n",
    "                         convTrans12,\n",
    "                         convTrans13) \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size, input_size = 256, shuffle = True):\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(root=\"yt-dls/train/\", transform = data_transforms['train_target'])\n",
    "    val_dataset = datasets.ImageFolder(root=\"yt-dls/val/\",  transform = data_transforms['train_target'])\n",
    "    test_dataset = datasets.ImageFolder(root=\"yt-dls/test/\",  transform = data_transforms['train_target'])\n",
    "\n",
    "    dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True),\n",
    "                       'val': torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = True),\n",
    "                       'test': torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)}\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0619f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl = get_dataloaders(5)\n",
    "#print(dl['train'])\n",
    "#video_folders = os.listdir(\"yt-dls/train/0\")\n",
    "#np.random.permutation(video_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e528110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(resnet, bottleneck, decoder, train_resnet):\n",
    "    # Get all the parameters\n",
    "    #print(\"bottleneck params\", bottleneck.parameters())\n",
    "    #print(\"decoder params\", decoder.parameters())\n",
    "    if train_resnet:       \n",
    "        params_to_update = list(resnet.parameters()) + list(bottleneck.parameters()) + list(decoder.parameters())\n",
    "    else:\n",
    "        params_to_update = list(bottleneck.parameters()) + list(decoder.parameters())\n",
    "\n",
    "    print(\"Params to learn:\")\n",
    "    for name, param in resnet.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    for name, param in bottleneck.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    for name, param in decoder.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    # Use SGD\n",
    "    #optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "    optimizer = optim.Adagrad(params_to_update, lr=0.01)\n",
    "    #optimizer = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07555ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_one_point(resnet, bottleneck, decoder, point, in_transforms, target_transforms, iterations=1000):\n",
    "    batched_input_1 = []\n",
    "    batched_input_2 = []\n",
    "    batched_targets = []\n",
    "    for example in point:  \n",
    "        in_1 = in_transforms(example[0].permute(2,0,1)) # frame_i-1\n",
    "        in_2 = in_transforms(example[1].permute(2,0,1)) # frame_i\n",
    "        target = target_transforms(example[1].permute(2,0,1))\n",
    "        batched_input_1.append(in_1)\n",
    "        batched_input_2.append(in_2)\n",
    "        batched_targets.append(target)\n",
    "    batched_input_1 = torch.stack(batched_input_1).to(device)\n",
    "    batched_input_2 = torch.stack(batched_input_2).to(device)\n",
    "    batched_targets = torch.stack(batched_targets).to(device)\n",
    "\n",
    "    optimizer = make_optimizer(bottleneck, decoder)\n",
    "    resnet.eval()\n",
    "    bottleneck.train()\n",
    "    decoder.train()\n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            embedding_1 = resnet(batched_input_1)\n",
    "            embedding_2 = resnet(batched_input_2)\n",
    "\n",
    "            cat_latent = torch.cat((embedding_1, embedding_2), dim=1)\n",
    "            moved_latent = torch.squeeze(cat_latent)\n",
    "            \n",
    "            latent = bottleneck(moved_latent)\n",
    "            \n",
    "            prediction = decoder(latent.unsqueeze(-1).unsqueeze(-1))\n",
    "            mse_loss = nn.MSELoss()\n",
    "            mse_loss_val = mse_loss(prediction, batched_targets)\n",
    "            loss = 1 * mse_loss_val\n",
    "            print(loss)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return [prediction, losses]\n",
    "\n",
    "def train(resnet, bottleneck, decoder, optimizer, data_transforms, train_resnet, num_epochs=25):\n",
    "    since = time.time()\n",
    "    mse_loss_history = []\n",
    "    best_model_wts = {'resnet':copy.deepcopy(resnet.state_dict()),\n",
    "                      'bottleneck':copy.deepcopy(bottleneck.state_dict()),\n",
    "                      'decoder':copy.deepcopy(decoder.state_dict())\n",
    "                     }\n",
    "    best_loss = 1000\n",
    "    val_loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs-1}\")\n",
    "        print('-'*10)\n",
    "        \n",
    "        # train and validate\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                if train_resnet:\n",
    "                    resnet.train()\n",
    "                bottleneck.train()\n",
    "                decoder.train()\n",
    "                root = \"yt-dls/train/0/\"\n",
    "                video_folders = np.random.permutation(os.listdir(root))\n",
    "                num_batches = len(video_folders)//5\n",
    "                #print(f\"num_batches:{num_batches}\")\n",
    "                batches = [[video_folders[5*i + j] for j in range(0,5)] for i in range(num_batches)]                \n",
    "            else:\n",
    "                resnet.eval()\n",
    "                bottleneck.eval()\n",
    "                decoder.eval()\n",
    "                root = \"yt-dls/val/0/\"\n",
    "                video_folders = np.random.permutation(os.listdir(root))\n",
    "                num_batches = len(video_folders)//5\n",
    "                #print(f\"num_batches:{num_batches}\")\n",
    "                batches = [[video_folders[5*i + j] for j in range(0,5)] for i in range(num_batches)]\n",
    "     \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            \n",
    "            for batch in tqdm(batches):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batched_input_1 = []\n",
    "                batched_input_2 = []\n",
    "                batched_targets = []\n",
    "                for video_folder in batch:\n",
    "                    # build up the batch\n",
    "                    #print(video_folder)\n",
    "                    num_frames = 30 # fit in memory batch * numframes\n",
    "                    frame_filenames = sorted(list(os.listdir(os.path.join(root, video_folder))))[0:num_frames]\n",
    "                    for i, frame_filename in enumerate(frame_filenames):\n",
    "                        #print(f\"i, {i}\")\n",
    "                        #print(f\"frame_filename, {frame_filename}\")\n",
    "                        frame_img = Image.open(os.path.join(root, video_folder, frame_filename))\n",
    "                        if i % 2 == 0:\n",
    "                            in_1 = data_transforms['train_in'](frame_img) # frame_i-1\n",
    "                            batched_input_1.append(in_1)\n",
    "                        else:\n",
    "                            in_2 = data_transforms['train_in'](frame_img)\n",
    "                            batched_input_2.append(in_2)\n",
    "                            target = data_transforms['train_target'](frame_img)\n",
    "                            #plt.imshow(transforms.functional.to_pil_image(target))\n",
    "                            #plt.show()\n",
    "                            batched_targets.append(target)\n",
    "                            \n",
    "                batched_input_1 = torch.stack(batched_input_1).to(device)\n",
    "                batched_input_2 = torch.stack(batched_input_2).to(device)\n",
    "                batched_targets = torch.stack(batched_targets).to(device)\n",
    "                \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    embedding_1 = resnet(batched_input_1)\n",
    "                    embedding_2 = resnet(batched_input_2)\n",
    "                    \n",
    "                    cat_latent = torch.cat((embedding_1, embedding_2), dim=1)\n",
    "                    moved_latent = torch.squeeze(cat_latent)\n",
    "\n",
    "                    latent = bottleneck(moved_latent)\n",
    "                    #print(latent.shape)\n",
    "                    #print((latent.unsqueeze(-1).unsqueeze(-1)).shape)\n",
    "\n",
    "                    prediction = decoder(latent.unsqueeze(-1).unsqueeze(-1))\n",
    "                    mse_loss = nn.MSELoss()\n",
    "                    mse_loss_val = mse_loss(prediction, batched_targets)\n",
    "                    loss = 1 * mse_loss_val\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            print(f\"{phase} Loss: {loss:.4f}\")\n",
    "            if phase == 'val' and loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model_wts = {'resnet':copy.deepcopy(resnet.state_dict()),\n",
    "                                  'bottleneck':copy.deepcopy(bottleneck.state_dict()),\n",
    "                                  'decoder':copy.deepcopy(decoder.state_dict())\n",
    "                                 }\n",
    "            if phase == 'val':\n",
    "                val_loss_history.append(loss.item())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    # save and load best model weights\n",
    "    torch.save(best_model_wts, os.path.join(\"saved\", 'weights_best.pt'))\n",
    "    \n",
    "    resnet.load_state_dict(best_model_wts['resnet'])\n",
    "    bottleneck.load_state_dict(best_model_wts['bottleneck'])\n",
    "    decoder.load_state_dict(best_model_wts['decoder'])\n",
    "    \n",
    "    return resnet, bottleneck, decoder, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2b4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a point\n",
    "# sample_video_filename = \"yt-dls/train/0/Secrets of the Whales _ Official Trailer _ Disney+.mp4\"\n",
    "# sample_video_filename2 = 'yt-dls/train/0/The Race For the COVID-19 Vaccine _ National Geographic.mp4'\n",
    "\n",
    "# sample_video = VideoClips([sample_video_filename], clip_length_in_frames=2, frames_between_clips=10, frame_rate=10)\n",
    "# sample_video2 = VideoClips([sample_video_filename2], clip_length_in_frames=2, frames_between_clips=10, frame_rate=10)\n",
    "\n",
    "# clip = sample_video.get_clip(0)\n",
    "# clip2 = sample_video2.get_clip(0)\n",
    "\n",
    "# batched_point = [clip[0], clip2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af529e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # fit a point\n",
    "# resnet, out_size = initialize_resnet_model()\n",
    "# resnet.to(device)\n",
    "# bottleneck = initialize_bottleneck(2*out_size).to(device)\n",
    "# decoder = initialize_decoder().to(device)\n",
    "# print(resnet)\n",
    "# print(bottleneck)\n",
    "# print(decoder)\n",
    "# last_prediction, losses = fit_one_point(resnet, bottleneck, decoder, batched_point, data_transforms['train_in'], data_transforms['train_target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5d3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57d114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_size: 512\n",
      "Each resnet is outputting 512 embedding.\n",
      "Params to learn:\n",
      "\t 0.weight\n",
      "\t 1.weight\n",
      "\t 1.bias\n",
      "\t 4.0.conv1.weight\n",
      "\t 4.0.bn1.weight\n",
      "\t 4.0.bn1.bias\n",
      "\t 4.0.conv2.weight\n",
      "\t 4.0.bn2.weight\n",
      "\t 4.0.bn2.bias\n",
      "\t 4.1.conv1.weight\n",
      "\t 4.1.bn1.weight\n",
      "\t 4.1.bn1.bias\n",
      "\t 4.1.conv2.weight\n",
      "\t 4.1.bn2.weight\n",
      "\t 4.1.bn2.bias\n",
      "\t 5.0.conv1.weight\n",
      "\t 5.0.bn1.weight\n",
      "\t 5.0.bn1.bias\n",
      "\t 5.0.conv2.weight\n",
      "\t 5.0.bn2.weight\n",
      "\t 5.0.bn2.bias\n",
      "\t 5.0.downsample.0.weight\n",
      "\t 5.0.downsample.1.weight\n",
      "\t 5.0.downsample.1.bias\n",
      "\t 5.1.conv1.weight\n",
      "\t 5.1.bn1.weight\n",
      "\t 5.1.bn1.bias\n",
      "\t 5.1.conv2.weight\n",
      "\t 5.1.bn2.weight\n",
      "\t 5.1.bn2.bias\n",
      "\t 6.0.conv1.weight\n",
      "\t 6.0.bn1.weight\n",
      "\t 6.0.bn1.bias\n",
      "\t 6.0.conv2.weight\n",
      "\t 6.0.bn2.weight\n",
      "\t 6.0.bn2.bias\n",
      "\t 6.0.downsample.0.weight\n",
      "\t 6.0.downsample.1.weight\n",
      "\t 6.0.downsample.1.bias\n",
      "\t 6.1.conv1.weight\n",
      "\t 6.1.bn1.weight\n",
      "\t 6.1.bn1.bias\n",
      "\t 6.1.conv2.weight\n",
      "\t 6.1.bn2.weight\n",
      "\t 6.1.bn2.bias\n",
      "\t 7.0.conv1.weight\n",
      "\t 7.0.bn1.weight\n",
      "\t 7.0.bn1.bias\n",
      "\t 7.0.conv2.weight\n",
      "\t 7.0.bn2.weight\n",
      "\t 7.0.bn2.bias\n",
      "\t 7.0.downsample.0.weight\n",
      "\t 7.0.downsample.1.weight\n",
      "\t 7.0.downsample.1.bias\n",
      "\t 7.1.conv1.weight\n",
      "\t 7.1.bn1.weight\n",
      "\t 7.1.bn1.bias\n",
      "\t 7.1.conv2.weight\n",
      "\t 7.1.bn2.weight\n",
      "\t 7.1.bn2.bias\n",
      "\t 0.weight\n",
      "\t 0.bias\n",
      "\t 1.weight\n",
      "\t 1.bias\n",
      "\t 0.0.weight\n",
      "\t 0.0.bias\n",
      "\t 0.1.weight\n",
      "\t 0.1.bias\n",
      "\t 1.0.weight\n",
      "\t 1.0.bias\n",
      "\t 1.1.weight\n",
      "\t 1.1.bias\n",
      "\t 2.0.weight\n",
      "\t 2.0.bias\n",
      "\t 2.1.weight\n",
      "\t 2.1.bias\n",
      "\t 3.0.weight\n",
      "\t 3.0.bias\n",
      "\t 3.1.weight\n",
      "\t 3.1.bias\n",
      "\t 4.0.weight\n",
      "\t 4.0.bias\n",
      "\t 4.1.weight\n",
      "\t 4.1.bias\n",
      "\t 5.0.weight\n",
      "\t 5.0.bias\n",
      "\t 5.1.weight\n",
      "\t 5.1.bias\n",
      "\t 6.0.weight\n",
      "\t 6.0.bias\n",
      "\t 6.1.weight\n",
      "\t 6.1.bias\n",
      "\t 7.0.weight\n",
      "\t 7.0.bias\n",
      "\t 7.1.weight\n",
      "\t 7.1.bias\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113854ab62ab402fa76b6fbe30f6a86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test training\n",
    "resnet, out_size = initialize_resnet_model()\n",
    "resnet.to(device)\n",
    "print(f\"Each resnet is outputting {out_size} embedding.\")\n",
    "bottleneck = initialize_bottleneck(2*out_size, latent_size = 2*out_size).to(device)\n",
    "decoder = initialize_decoder().to(device)\n",
    "optimizer = make_optimizer(resnet, bottleneck, decoder, train_resnet = True)\n",
    "# get 5 clips at a time? how many frames each?\n",
    "dataloaders_dict = get_dataloaders(3)\n",
    "print(resnet)\n",
    "print(bottleneck)\n",
    "print(decoder)\n",
    "resnet_result, bottleneck_result, decoder_result, val_loss_history = train(resnet, bottleneck, decoder, optimizer, data_transforms, True, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cd610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view testing data\n",
    "resnet_result.eval()\n",
    "bottleneck_result.eval()\n",
    "decoder_result.eval()\n",
    "\n",
    "def test_visualize(resnet, bottleneck, decoder):\n",
    "    root = \"yt-dls/test/0/\"\n",
    "    video_folders = np.random.permutation(os.listdir(root))\n",
    "    print(video_folders)\n",
    "    batched_input_1 = []\n",
    "    batched_input_2 = []\n",
    "    batched_targets = []\n",
    "    for video_folder in video_folders:\n",
    "        # build up the batch\n",
    "        #print(video_folder)\n",
    "        num_frames = 30 # fit in memory batch * numframes\n",
    "        frame_filenames = sorted(list(os.listdir(os.path.join(root, video_folder))))[0:num_frames]\n",
    "        for i, frame_filename in enumerate(frame_filenames):\n",
    "            #print(f\"i, {i}\")\n",
    "            #print(f\"frame_filename, {frame_filename}\")\n",
    "            frame_img = Image.open(os.path.join(root, video_folder, frame_filename))\n",
    "            #plt.imshow(frame_img)\n",
    "            #plt.show()\n",
    "            if i % 2 == 0:\n",
    "                in_1 = data_transforms['train_in'](frame_img) # frame_i-1\n",
    "                batched_input_1.append(in_1)\n",
    "            else:\n",
    "                in_2 = data_transforms['train_in'](frame_img)\n",
    "                batched_input_2.append(in_2)\n",
    "                target = data_transforms['train_target'](frame_img)\n",
    "                batched_targets.append(target)\n",
    "    batched_input_1 = torch.stack(batched_input_1).to(device)\n",
    "    batched_input_2 = torch.stack(batched_input_2).to(device)\n",
    "    batched_targets = torch.stack(batched_targets).to(device)\n",
    "\n",
    "    embedding_1 = resnet(batched_input_1)\n",
    "    embedding_2 = resnet(batched_input_2)\n",
    "\n",
    "    cat_latent = torch.cat((embedding_1, embedding_2), dim=1)\n",
    "    moved_latent = torch.squeeze(cat_latent)\n",
    "\n",
    "    latent = bottleneck(moved_latent)\n",
    "\n",
    "    prediction = decoder(latent.unsqueeze(-1).unsqueeze(-1))\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mse_loss_val = mse_loss(prediction, batched_targets)\n",
    "    loss = 1 * mse_loss_val\n",
    "\n",
    "    print(loss)\n",
    "\n",
    "    print(prediction.shape)\n",
    "    print(target.shape)\n",
    "    for i in range(len(prediction)):\n",
    "        plt.imshow(transforms.functional.to_pil_image(prediction[i].cpu()))\n",
    "        print(\"prediction\")\n",
    "        plt.show()\n",
    "        plt.imshow(transforms.functional.to_pil_image(batched_targets[i].cpu()))\n",
    "        print(\"target\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9ee68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_visualize(resnet_result, bottleneck_result, decoder_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac13442",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = last_prediction[1].cpu().detach()\n",
    "target = batched_point[1][1]\n",
    "input_target = data_transforms['train_in_no_normalize'](batched_point[1][1].permute(2,0,1))\n",
    "print(prediction.shape)\n",
    "#prediction_img = transforms.ToPILImage(prediction)\n",
    "print(target.shape)\n",
    "\n",
    "print(\"The original target frame\")\n",
    "plt.imshow(target)\n",
    "plt.show()\n",
    "\n",
    "transformed_target = data_transforms['train_target'](target.permute(2,0,1))\n",
    "print(\"the target to optimize for\")\n",
    "plt.imshow(transformed_target.permute(1,2,0))\n",
    "plt.show()\n",
    "print(\"what the model saw\")\n",
    "plt.imshow(transforms.functional.to_pil_image(input_target))\n",
    "plt.show()\n",
    "print(\"what the model made\")\n",
    "plt.imshow(transforms.functional.to_pil_image(prediction))\n",
    "plt.show()\n",
    "print(\"bilinear baseline\")\n",
    "plt.imshow(transforms.functional.resize(transforms.functional.to_pil_image(input_target), 512))\n",
    "#plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
