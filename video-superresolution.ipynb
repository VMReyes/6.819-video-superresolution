{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f8943e",
   "metadata": {},
   "source": [
    "# Video Super Resolution\n",
    "\n",
    "## By Daniel Shkreli and Victor Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd443c",
   "metadata": {},
   "source": [
    "### Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a277c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.57.0)\n",
      "Using the GPU!\n",
      "1.4.0\n",
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import lpips\n",
    "import torchvision\n",
    "from torchvision.datasets.video_utils import VideoClips\n",
    "from torchvision import datasets, models, transforms\n",
    "!pip install tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")\n",
    "    \n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b884ed7",
   "metadata": {},
   "source": [
    "### Import the dataset\n",
    "To do this, we will be downloading the videos of the National Geographic channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c00196",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We will use resnet 18, pretrained at first.\n",
    "from https://github.com/hsinyilin19/ResNetVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0ee5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256\n",
    "data_transforms = {\n",
    "    'train_in': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'train_in_no_normalize': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'train_target': transforms.Compose([\n",
    "        transforms.Resize(input_size*2),\n",
    "        transforms.CenterCrop(input_size*2),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "def initialize_resnet_model(resume_from = None):\n",
    "    resnet = models.resnet18(pretrained=True)\n",
    "    out_size = resnet.fc.in_features\n",
    "    print(\"out_size:\", out_size)\n",
    "    layers = []\n",
    "    for child in resnet.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "        layers.append(child)\n",
    "    resnet = nn.Sequential(*layers[:-1])\n",
    "    \n",
    "    #resnet.fc =  # create latent\n",
    "    \n",
    "    #resnet.add_module(\"bottleneck\", nn.Sequential()\n",
    "    if resume_from is not None:\n",
    "        print(f\"Loading weights from {resume_from}\")\n",
    "        model.load_state_dict(torch.load(resume_from))\n",
    "    return resnet, out_size\n",
    "\n",
    "def initialize_bottleneck(input_size, latent_size=1024):\n",
    "    bottleneck = nn.Sequential(nn.Linear(input_size, latent_size),\n",
    "                               nn.BatchNorm1d(latent_size, momentum=0.01))\n",
    "    return bottleneck\n",
    "\n",
    "def initialize_decoder():\n",
    "    stride = (2,2)\n",
    "    padding = (1,1)\n",
    "    convTrans6 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=1024, out_channels=1024, kernel_size=(4,4), stride=(1,1),\n",
    "                           padding=(0,0)),\n",
    "        nn.BatchNorm2d(1024, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans7 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(512, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans8 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(256, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans9 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(128, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans10 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(64, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "   \n",
    "    convTrans11 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(64, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans12 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(32, momentum=0.01),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    convTrans13 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=(4,4), stride=stride,\n",
    "                           padding=padding),\n",
    "        nn.BatchNorm2d(3, momentum=0.01),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "    return nn.Sequential(convTrans6,\n",
    "                         convTrans7,\n",
    "                         convTrans8,\n",
    "                         convTrans9,\n",
    "                         convTrans10,\n",
    "                         convTrans11,\n",
    "                         convTrans12,\n",
    "                         convTrans13) \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size, input_size = 256, shuffle = True):\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(root=\"yt-dls/train/\", transform = data_transforms['train_target'])\n",
    "    val_dataset = datasets.ImageFolder(root=\"yt-dls/val/\",  transform = data_transforms['train_target'])\n",
    "    test_dataset = datasets.ImageFolder(root=\"yt-dls/test/\",  transform = data_transforms['train_target'])\n",
    "\n",
    "    dataloaders_dict = {'train': torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True),\n",
    "                       'val': torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = True),\n",
    "                       'test': torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)}\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0619f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Stingray_Stunner-Full_Episode-Secrets_of_the_Zoo.mp4',\n",
       "       'Mapping_the_Green_Book_National_Geographic',\n",
       "       'The_Water_Crisis_National_Geographic.mp4',\n",
       "       'Anthony_Mackie_Descends_a_Cliff_Face_Running_Wild_with_Bear_Grylls',\n",
       "       'New_Zealand_s_Stunning_Landscapes_National_Geographic',\n",
       "       'Fly_Like_an_Eagle-Full_Episode-Alaska_Animal_Rescue',\n",
       "       'The_First_Militaristic_Drug_Cartel_Narco_Wars',\n",
       "       'Narcotics_Hidden_in_a_Fan_To_Catch_a_Smuggler',\n",
       "       'The_Secret_of_Musical_Genius_Podcast_Overheard_at_National_Geographic.mp4',\n",
       "       'Year_of_the_Dog-Full_Episode-Dog_Whisperer_with_Cesar_Millan',\n",
       "       'Whole_Llama_Love-Full_Episode-Heartland_Docs,_DVM',\n",
       "       'The_Quiet_Beauty_of_Kaikoura_National_Geographic.mp4',\n",
       "       'The_Evolution_of_the_Vigilante_The_March_on_Washington.mp4',\n",
       "       'The_Secret_Culture_of_Orcas_Podcast_Overheard_at_National_Geographic.mp4',\n",
       "       'The_Rise_of_the_Cali_Drug_Cartel_Narco_Wars.mp4',\n",
       "       'Legends_of_Kingfishers,_Otters,_and_Red-tailed_Hawks_Podcast_Overheard_at_National_Geographic',\n",
       "       'The_Boar_Ultimatum-Full_Episode-Dr._Oakley,_Yukon_Vet.mp4',\n",
       "       'Anthony_Mackie_Descends_a_Cliff_Face_Running_Wild_with_Bear_Grylls.mp4',\n",
       "       'Blood_Warriors-Full_Episode-Primal_Survivor',\n",
       "       'Legends_of_Kingfishers,_Otters,_and_Red-tailed_Hawks_Podcast_Overheard_at_National_Geographic.mp4',\n",
       "       'The_Real_Amazons_Podcast_Overheard_at_National_Geographic.mp4',\n",
       "       'New_Zealand_s_Stunning_Landscapes_National_Geographic.mp4',\n",
       "       'Yellowstone-Full_Episode-America_s_National_Parks.mp4',\n",
       "       'The_Rise_of_the_Cali_Drug_Cartel_Narco_Wars',\n",
       "       'Dawn_of_Darkness-Full_Episode-Savage_Kingdom',\n",
       "       'Run_Cheetah_Run-Full_Episode-Secrets_of_the_Zoo.mp4',\n",
       "       'The_Real_Amazons_Podcast_Overheard_at_National_Geographic',\n",
       "       'Hauling_a_Canoe_Up_a_Cliff_Race_to_the_Center_of_the_Earth',\n",
       "       'Hauling_a_Canoe_Up_a_Cliff_Race_to_the_Center_of_the_Earth.mp4',\n",
       "       'Own_the_Room_Official_Trailer_Disney+.mp4',\n",
       "       'Adapt_or_Die-Full_Episode-Life_Below_Zero.mp4',\n",
       "       'The_Tormentor-Full_Episode-Dog_Impossible.mp4',\n",
       "       'The_Quiet_Beauty_of_Kaikoura_National_Geographic',\n",
       "       'Mamba_in_My_Closet-Full_Episode-Snake_City',\n",
       "       'Aretha_Franklin_Finds_Her_Sound_Genius-Aretha.mp4',\n",
       "       'Genius-Aretha_Chain_of_Fools_Trailer_National_Geographic',\n",
       "       'The_Tormentor-Full_Episode-Dog_Impossible',\n",
       "       'The_First_Militaristic_Drug_Cartel_Narco_Wars.mp4',\n",
       "       'Breaking_Bobby_Bones_Trailer_National_Geographic.mp4',\n",
       "       'Red_Summer_Trailer_National_Geographic.mp4',\n",
       "       'Mountains-Full_Episode-Hostile_Planet',\n",
       "       'Breaking_Bobby_Bones_Trailer_National_Geographic',\n",
       "       'The_Secret_of_Musical_Genius_Podcast_Overheard_at_National_Geographic',\n",
       "       'How_the_Mojave_Desert_Compares_to_Mars_National_Geographic.mp4',\n",
       "       'Mamba_in_My_Closet-Full_Episode-Snake_City.mp4',\n",
       "       'StarTalk_with_Neil_deGrasse_Tyson_and_Stephen_Hawking_Full_Episode',\n",
       "       'How_One_Community_Saved_Its_Fish_National_Geographic',\n",
       "       'Earth_Day_Eve_2021_National_Geographic',\n",
       "       'The_Race_For_the_COVID-19_Vaccine_National_Geographic',\n",
       "       'Red_Summer_Trailer_National_Geographic',\n",
       "       'Three_People_Rappel_Down_a_Cliff_Race_to_the_Center_of_the_Earth.mp4',\n",
       "       'Own_the_Room_Official_Trailer_Disney+',\n",
       "       'Genius-Aretha_Chain_of_Fools_Trailer_National_Geographic.mp4',\n",
       "       'What_s_Moo_with_Ewe-Full_Episode-The_Incredible_Dr._Pol',\n",
       "       'Earth_Day_Eve_2021_National_Geographic.mp4',\n",
       "       'The_Evolution_of_the_Vigilante_The_March_on_Washington',\n",
       "       'The_Chihuahua_and_the_Chicken-Full_Episode-Unlikely_Animal_Friends',\n",
       "       'Adapt_or_Die-Full_Episode-Life_Below_Zero',\n",
       "       'The_Chihuahua_and_the_Chicken-Full_Episode-Unlikely_Animal_Friends.mp4',\n",
       "       'Fly_Like_an_Eagle-Full_Episode-Alaska_Animal_Rescue.mp4',\n",
       "       'StarTalk_with_Neil_deGrasse_Tyson_and_Stephen_Hawking_Full_Episode.mp4',\n",
       "       'Mountains-Full_Episode-Hostile_Planet.mp4',\n",
       "       'The_Secret_Culture_of_Orcas_Podcast_Overheard_at_National_Geographic',\n",
       "       'Year_of_the_Dog-Full_Episode-Dog_Whisperer_with_Cesar_Millan.mp4',\n",
       "       'Stingray_Stunner-Full_Episode-Secrets_of_the_Zoo',\n",
       "       'The_Race_For_the_COVID-19_Vaccine_National_Geographic.mp4',\n",
       "       'How_the_Mojave_Desert_Compares_to_Mars_National_Geographic',\n",
       "       'Yellowstone-Full_Episode-America_s_National_Parks',\n",
       "       'Mapping_the_Green_Book_National_Geographic.mp4',\n",
       "       'Aretha_Franklin_Finds_Her_Sound_Genius-Aretha',\n",
       "       'Blood_Warriors-Full_Episode-Primal_Survivor.mp4',\n",
       "       'The_Boar_Ultimatum-Full_Episode-Dr._Oakley,_Yukon_Vet',\n",
       "       'Dawn_of_Darkness-Full_Episode-Savage_Kingdom.mp4',\n",
       "       'What_s_Moo_with_Ewe-Full_Episode-The_Incredible_Dr._Pol.mp4',\n",
       "       'The_Water_Crisis_National_Geographic',\n",
       "       'Whole_Llama_Love-Full_Episode-Heartland_Docs,_DVM.mp4',\n",
       "       'Three_People_Rappel_Down_a_Cliff_Race_to_the_Center_of_the_Earth',\n",
       "       'Run_Cheetah_Run-Full_Episode-Secrets_of_the_Zoo',\n",
       "       'How_One_Community_Saved_Its_Fish_National_Geographic.mp4',\n",
       "       'Narcotics_Hidden_in_a_Fan_To_Catch_a_Smuggler.mp4'], dtype='<U97')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = get_dataloaders(5)\n",
    "#print(dl['train'])\n",
    "video_folders = os.listdir(\"yt-dls/train/0\")\n",
    "np.random.permutation(video_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e528110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # TQDM has nice progress bars\n",
    "            for inputs, _ in tqdm(dataloaders[phase]):\n",
    "                #inputs2 = data_transforms['train'](inputs)\n",
    "                # we get 10, 720, 1280, 3\n",
    "                # for now, lets just get a single frame\n",
    "                # perform transform on input\n",
    "                transformed_input = data_transforms[phase](inputs)\n",
    "                label = inputs # this makes sense, should we perform any transforms?\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    prediction = model(inputs)\n",
    "                    preceptual_loss = percept(prediction, label)\n",
    "                    mse_loss = nn.mse(prediction, label)\n",
    "                    loss = alpha * perceptual_loss + beta * mse_loss\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                print(inputs2.shape)\n",
    "\n",
    "def make_optimizer(bottleneck, decoder):\n",
    "    # Get all the parameters\n",
    "    print(\"bottleneck params\", bottleneck.parameters())\n",
    "    print(\"decoder params\", decoder.parameters())\n",
    "    params_to_update = list(bottleneck.parameters()) + list(decoder.parameters())\n",
    "    print(\"Params to learn:\")\n",
    "    for name, param in bottleneck.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    for name, param in decoder.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    # Use SGD\n",
    "    #optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    optimizer = optim.Adagrad(params_to_update, lr=0.001)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07555ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_one_point(resnet, bottleneck, decoder, point, in_transforms, target_transforms, iterations=1000):\n",
    "    batched_input_1 = []\n",
    "    batched_input_2 = []\n",
    "    batched_targets = []\n",
    "    for example in point:  \n",
    "        in_1 = in_transforms(example[0].permute(2,0,1)) # frame_i-1\n",
    "        in_2 = in_transforms(example[1].permute(2,0,1)) # frame_i\n",
    "        target = target_transforms(example[1].permute(2,0,1))\n",
    "        batched_input_1.append(in_1)\n",
    "        batched_input_2.append(in_2)\n",
    "        batched_targets.append(target)\n",
    "    batched_input_1 = torch.stack(batched_input_1).to(device)\n",
    "    batched_input_2 = torch.stack(batched_input_2).to(device)\n",
    "    batched_targets = torch.stack(batched_targets).to(device)\n",
    "\n",
    "    optimizer = make_optimizer(bottleneck, decoder)\n",
    "    resnet.eval()\n",
    "    bottleneck.train()\n",
    "    decoder.train()\n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            embedding_1 = resnet(batched_input_1)\n",
    "            embedding_2 = resnet(batched_input_2)\n",
    "\n",
    "            cat_latent = torch.cat((embedding_1, embedding_2), dim=1)\n",
    "            moved_latent = torch.squeeze(cat_latent)\n",
    "            \n",
    "            latent = bottleneck(moved_latent)\n",
    "            \n",
    "            prediction = decoder(latent.unsqueeze(-1).unsqueeze(-1))\n",
    "            mse_loss = nn.MSELoss()\n",
    "            mse_loss_val = mse_loss(prediction, batched_targets)\n",
    "            loss = 1 * mse_loss_val\n",
    "            print(loss)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return [prediction, losses]\n",
    "\n",
    "def train(resnet, bottleneck, decoder, optimizer, transforms, num_epochs=25):\n",
    "    since = time.time()\n",
    "    mse_loss_history = []\n",
    "    best_model_wts = {'resnet':copy.deepcopy(resnet.state_dict()),\n",
    "                      'bottleneck':copy.deepcopy(bottleneck.state_dict()),\n",
    "                      'decoder':copy.deepcopy(decoder.state_dict())\n",
    "                     }\n",
    "    best_loss = 1000\n",
    "    val_loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs-1}\")\n",
    "        print('-'*10)\n",
    "        \n",
    "        # train and validate\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                bottleneck.train()\n",
    "                decoder.train()\n",
    "                root = \"yt-dls/train/0/\"\n",
    "                video_folders = np.random.permutation(os.listdir(root))\n",
    "                batches = [[video_folders[5*i + j] for j in range(0,5)] for i in range(5)]                \n",
    "            else:\n",
    "                bottleneck.eval()\n",
    "                decoder.eval()\n",
    "                root = \"yt-dls/val/0/\"\n",
    "                video_folders = np.random.permutation(os.listdir(root))\n",
    "                batches = [[video_folders[5*i + j] for j in range(0,5)] for i in range(5)]\n",
    "     \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            \n",
    "            for batch in tqdm(batches):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batched_input_1 = []\n",
    "                batched_input_2 = []\n",
    "                batched_targets = []\n",
    "                for video_folder in batch:\n",
    "                    # build up the batch\n",
    "                    #print(video_folder)\n",
    "                    num_frames = 10 # fit in memory batch * numframes\n",
    "                    frame_filenames = sorted(list(os.listdir(os.path.join(root, video_folder))))[0:num_frames]\n",
    "                    for i, frame_filename in enumerate(frame_filenames):\n",
    "                        #print(f\"i, {i}\")\n",
    "                        #print(f\"frame_filename, {frame_filename}\")\n",
    "                        frame_img = Image.open(os.path.join(root, video_folder, frame_filename))\n",
    "                        if i % 2 == 0:\n",
    "                            in_1 = transforms['train_in'](frame_img) # frame_i-1\n",
    "                            batched_input_1.append(in_1)\n",
    "                        else:\n",
    "                            in_2 = transforms['train_in'](frame_img)\n",
    "                            batched_input_2.append(in_2)\n",
    "                            target = transforms['train_target'](frame_img)\n",
    "                            batched_targets.append(target)\n",
    "                batched_input_1 = torch.stack(batched_input_1).to(device)\n",
    "                batched_input_2 = torch.stack(batched_input_2).to(device)\n",
    "                batched_targets = torch.stack(batched_targets).to(device)\n",
    "                \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    embedding_1 = resnet(batched_input_1)\n",
    "                    embedding_2 = resnet(batched_input_2)\n",
    "                    \n",
    "                    cat_latent = torch.cat((embedding_1, embedding_2), dim=1)\n",
    "                    moved_latent = torch.squeeze(cat_latent)\n",
    "\n",
    "                    latent = bottleneck(moved_latent)\n",
    "\n",
    "                    prediction = decoder(latent.unsqueeze(-1).unsqueeze(-1))\n",
    "                    mse_loss = nn.MSELoss()\n",
    "                    mse_loss_val = mse_loss(prediction, batched_targets)\n",
    "                    loss = 1 * mse_loss_val\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            print(f\"{phase} Loss: {loss:.4f}\")\n",
    "            if phase == 'val' and loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = {'resnet':copy.deepcopy(resnet.state_dict()),\n",
    "                                  'bottleneck':copy.deepcopy(bottleneck.state_dict()),\n",
    "                                  'decoder':copy.deepcopy(decoder.state_dict())\n",
    "                                 }\n",
    "            if phase == 'val':\n",
    "                val_loss_history.append(loss.item())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    # save and load best model weights\n",
    "    torch.save(best_model_wts, os.path.join(save_dir, 'weights_best.pt'))\n",
    "    \n",
    "    resnet.load_state_dict(best_model_wts['resnet'])\n",
    "    bottleneck.load_state_dict(best_model_wts['bottleneck'])\n",
    "    decoder.load_state_dict(best_model_wts['decoder'])\n",
    "    \n",
    "    return resnet, bottleneck, decoder, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2b4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a point\n",
    "# sample_video_filename = \"yt-dls/train/0/Secrets of the Whales _ Official Trailer _ Disney+.mp4\"\n",
    "# sample_video_filename2 = 'yt-dls/train/0/The Race For the COVID-19 Vaccine _ National Geographic.mp4'\n",
    "\n",
    "# sample_video = VideoClips([sample_video_filename], clip_length_in_frames=2, frames_between_clips=10, frame_rate=10)\n",
    "# sample_video2 = VideoClips([sample_video_filename2], clip_length_in_frames=2, frames_between_clips=10, frame_rate=10)\n",
    "\n",
    "# clip = sample_video.get_clip(0)\n",
    "# clip2 = sample_video2.get_clip(0)\n",
    "\n",
    "# batched_point = [clip[0], clip2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af529e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # fit a point\n",
    "# resnet, out_size = initialize_resnet_model()\n",
    "# resnet.to(device)\n",
    "# bottleneck = initialize_bottleneck(2*out_size).to(device)\n",
    "# decoder = initialize_decoder().to(device)\n",
    "# print(resnet)\n",
    "# print(bottleneck)\n",
    "# print(decoder)\n",
    "# last_prediction, losses = fit_one_point(resnet, bottleneck, decoder, batched_point, data_transforms['train_in'], data_transforms['train_target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a5d3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed57d114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_size: 512\n",
      "bottleneck params <generator object Module.parameters at 0x7f52049de5c8>\n",
      "decoder params <generator object Module.parameters at 0x7f52049de5c8>\n",
      "Params to learn:\n",
      "\t 0.weight\n",
      "\t 0.bias\n",
      "\t 1.weight\n",
      "\t 1.bias\n",
      "\t 0.0.weight\n",
      "\t 0.0.bias\n",
      "\t 0.1.weight\n",
      "\t 0.1.bias\n",
      "\t 1.0.weight\n",
      "\t 1.0.bias\n",
      "\t 1.1.weight\n",
      "\t 1.1.bias\n",
      "\t 2.0.weight\n",
      "\t 2.0.bias\n",
      "\t 2.1.weight\n",
      "\t 2.1.bias\n",
      "\t 3.0.weight\n",
      "\t 3.0.bias\n",
      "\t 3.1.weight\n",
      "\t 3.1.bias\n",
      "\t 4.0.weight\n",
      "\t 4.0.bias\n",
      "\t 4.1.weight\n",
      "\t 4.1.bias\n",
      "\t 5.0.weight\n",
      "\t 5.0.bias\n",
      "\t 5.1.weight\n",
      "\t 5.1.bias\n",
      "\t 6.0.weight\n",
      "\t 6.0.bias\n",
      "\t 6.1.weight\n",
      "\t 6.1.bias\n",
      "\t 7.0.weight\n",
      "\t 7.0.bias\n",
      "\t 7.1.weight\n",
      "\t 7.1.bias\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (1): BatchNorm1d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(3, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53da317b3334b84b029eddb54af25d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 11.17 GiB total capacity; 10.64 GiB already allocated; 76.44 MiB free; 10.82 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-87ac6c78422c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-d30665d5e504>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(resnet, bottleneck, decoder, optimizer, transforms, num_epochs)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoved_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0mmse_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mmse_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 11.17 GiB total capacity; 10.64 GiB already allocated; 76.44 MiB free; 10.82 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# test training\n",
    "resnet, out_size = initialize_resnet_model()\n",
    "resnet.to(device)\n",
    "bottleneck = initialize_bottleneck(2*out_size).to(device)\n",
    "decoder = initialize_decoder().to(device)\n",
    "optimizer = make_optimizer(bottleneck, decoder)\n",
    "# get 5 clips at a time? how many frames each?\n",
    "dataloaders_dict = get_dataloaders(3)\n",
    "print(resnet)\n",
    "print(bottleneck)\n",
    "print(decoder)\n",
    "resnet, bottleneck, decoder, val_loss_history = train(resnet, bottleneck, decoder, optimizer, data_transforms, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac13442",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = last_prediction[1].cpu().detach()\n",
    "target = batched_point[1][1]\n",
    "input_target = data_transforms['train_in_no_normalize'](batched_point[1][1].permute(2,0,1))\n",
    "print(prediction.shape)\n",
    "#prediction_img = transforms.ToPILImage(prediction)\n",
    "print(target.shape)\n",
    "\n",
    "print(\"The original target frame\")\n",
    "plt.imshow(target)\n",
    "plt.show()\n",
    "\n",
    "transformed_target = data_transforms['train_target'](target.permute(2,0,1))\n",
    "print(\"the target to optimize for\")\n",
    "plt.imshow(transformed_target.permute(1,2,0))\n",
    "plt.show()\n",
    "print(\"what the model saw\")\n",
    "plt.imshow(transforms.functional.to_pil_image(input_target))\n",
    "plt.show()\n",
    "print(\"what the model made\")\n",
    "plt.imshow(transforms.functional.to_pil_image(prediction))\n",
    "plt.show()\n",
    "print(\"bilinear baseline\")\n",
    "plt.imshow(transforms.functional.resize(transforms.functional.to_pil_image(input_target), 512))\n",
    "#plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb29fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dict = get_dataloaders(1)\n",
    "#print(dataloaders_dict)\n",
    "\n",
    "point = dataloaders_dict['train']\n",
    "\n",
    "\n",
    "# overfit a point\n",
    "print(point)\n",
    "since = time.time()\n",
    "\n",
    "val_acc_history = []\n",
    "\n",
    "best_model_wts = copy.deepcopy(resnet.state_dict())\n",
    "best_loss = 0.0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    resnet.train()\n",
    "    # Each epoch has a training and validation phase\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    # TQDM has nice progress bars\n",
    "    for inputs, _ in tqdm(dataloaders[phase]):\n",
    "        #inputs2 = data_transforms['train'](inputs)\n",
    "        # we get 10, 720, 1280, 3\n",
    "        # for now, lets just get a single frame\n",
    "        # perform transform on input\n",
    "        transformed_input = data_transforms[phase](inputs)\n",
    "        label = inputs # this makes sense, should we perform any transforms?\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            prediction = model(inputs)\n",
    "            preceptual_loss = percept(prediction, label)\n",
    "            mse_loss = nn.mse(prediction, label)\n",
    "            loss = alpha * perceptual_loss + beta * mse_loss\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(inputs2.shape)\n",
    "\n",
    "\n",
    "for inputs, _ in tqdm(dataloaders_dict['val']):\n",
    "    print(inputs.shape)\n",
    "    inputs2 = data_transforms['train'](inputs)\n",
    "\n",
    "    print(inputs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fd956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parking Lot Drug Bust _ To Catch a Smuggler.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569c1e4ce2cc434c888476912183a65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/io/video.py:107: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"follow-up version. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why War Zones Need Science _ Podcast _ Overheard at National Geographic.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b517986b61d84900ba5236b842c95a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad2301f1f8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mclips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoClips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yt-dls/train/0/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_length_in_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_between_clips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_clip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmin_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/datasets/video_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, video_paths, clip_length_in_frames, frames_between_clips, frame_rate, _precomputed_metadata, num_workers, _video_width, _video_height, _video_min_dimension, _audio_samples, _audio_channels)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_precomputed_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_frame_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_precomputed_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/datasets/video_utils.py\u001b[0m in \u001b[0;36m_compute_frame_pts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mclips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/datasets/video_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mread_video_timestamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/io/video.py\u001b[0m in \u001b[0;36mread_video_timestamps\u001b[0;34m(filename, pts_unit)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 video_frames = _read_from_stream(container, 0, float(\"inf\"), pts_unit,\n\u001b[0;32m--> 320\u001b[0;31m                                                  video_stream, {'video': 0})\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mvideo_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/io/video.py\u001b[0m in \u001b[0;36m_read_from_stream\u001b[0;34m(container, start_offset, end_offset, pts_unit, stream, stream_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mbuffer_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpts\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# precompute videoClips\n",
    "# find min clip # and extract that amount from all\n",
    "min_clip = 100000\n",
    "for filename in os.listdir(\"yt-dls/train/0\"):\n",
    "    print(filename)\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        clips = VideoClips([\"yt-dls/train/0/\"+filename], clip_length_in_frames=2, frames_between_clips = 1200, frame_rate=10)\n",
    "        if clips.num_clips() < min_clip:\n",
    "            min_clip = clips.num_clips()\n",
    "        del clips\n",
    "        gc.collect()\n",
    "print(min_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0e9c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract min_clip from all to use for training\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
